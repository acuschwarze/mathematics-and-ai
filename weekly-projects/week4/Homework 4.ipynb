{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3370b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 4 (due 07/24/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58735d16-cb06-47ba-ac58-c608b9d06f78",
   "metadata": {},
   "source": [
    "# Decision trees, interpretability, and algorithmic bias\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this week's project, you will explore the COMPAS data set. COMPAS stands for \"Correctional Offender Management Profiling for Alternative Sanctions\". It is a software/algorithm that is used to assess the risk of a registered offender is going to commit another offense. Although researchers and journalists have pointed to [various problems](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) of this algorithm over many years, the algorithm is still used to inform sentences and parole decisions in several US states. \n",
    "You can learn more about the COMPAS data set [here](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis). \n",
    "\n",
    "Through this project, you will practice fitting and validating several classification models and you will explore some distinct benefits of using decision trees in machine learning. As part of that exploration, you are going to audit your model for demographic biases via a \"closed box\" and an \"open box\" approach.\n",
    "\n",
    "The COMPAS data set is a favorite example among critics of machine learning because it demonstrates several shortcomings and failure modes of machine learning techniques. The lessons learned from this project might be discouraging, and they are important. Keep in mind, however, that what you see here does not generalize to all data sets. \n",
    "\n",
    "This project has four parts.\n",
    "\n",
    "### Part 1: Prepare the COMPAS data set  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "In this part, you will load the COMPAS data set, explore its content, and select several variables as features (i.e., queries) or class labels (i.e., responses). Some of these features are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include categorical variable with more than two categories. You will uses 1-hot encoding to include this feature in your data set. \n",
    "\n",
    "This part includes four steps:\n",
    "1. Load and explore data set\n",
    "2. Select features and response variables\n",
    "3. Construct numerical coding for categorical features\n",
    "4. Split the data\n",
    "\n",
    "### Part 2: Train and validate a decision tree  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "In this part, you will fit a decision tree to your data. You will examine the effect of tuning the complexity of the tree via the \"maximum number of leaves\" parameter and use 5-fold cross-validation to find an optimal value.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit a decision tree on the training data\n",
    "2. Tune the parameter \"maximum number of leaves\"\n",
    "3. Calculate the selected model's test performance\n",
    "\n",
    "\n",
    "### Part 3: Auditing a decision tree for demographic biases  (PARTIALLY YOU TO COMPLETE)\n",
    "\n",
    "Your training data includes several demographic variables (i.e., age, sex, race). A crude way to assess whether a model has some demographic bias is to remove the corresponding variables from your training data and explore how that removal affects your model's performance. Decision trees have the advantage of being interpretable machine learning models. By going through the decision nodes (i.e., branching points), you can \"open the black box and look inside\". Specifically, you can assess how each feature is used in the decision making process.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit a decision tree\n",
    "2. Check for racial bias via performance assessment\n",
    "3. Check for racial bias via decision rules\n",
    "\n",
    "### Part 4: Comparison to other linear classifiers (FOR YOU TO COMPLETE)\n",
    "\n",
    "For some types of data, decision trees tend to achieve lower prediction accuracies In this part, you will train and tune several classifiers on the COMPAS data. You will then compare their performance on your test set.\n",
    "\n",
    "This part includes three steps:\n",
    "\n",
    "1. Fit LDA and logistic regression\n",
    "2. Tune and fit ensemble methods\n",
    "3. Tune and fit SVC\n",
    "4. Compare performance metrics for all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bf61d6ee-6086-420a-a1b3-9002b2d292b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b14b03-f395-4bb0-abce-f67293c4a5db",
   "metadata": {},
   "source": [
    "## Part 1: Prepare the COMPAS data set\n",
    "\n",
    ">In this part, you will load the COMPAS data set, explore its content, and select several variables as features (i.e., queries) or class labels (i.e., responses). Some of these features are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include categorical variable with more than two categories. You will uses 1-hot encoding to include this feature in your data set.\n",
    ">\n",
    ">This part includes four steps:\n",
    ">1. Load and explore data set\n",
    ">2. Select features and response variables\n",
    ">3. Construct numerical coding for categorical features\n",
    ">4. Split the data\n",
    "\n",
    "\n",
    "\n",
    "### Part 1, Step 1: Load and explore data set\n",
    "\n",
    "This folder includes the 'compas-scores-two-years.csv' file. The COMPAS data that you will use for this project is in this file. It is always a good idea to look at the raw data before proceeding with one's machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1f35e4e7-8eab-44b2-a03f-5ed100f53516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'first', 'last', 'compas_screening_date', 'sex', 'dob',\n",
      "       'age', 'age_cat', 'race', 'juv_fel_count', 'decile_score',\n",
      "       'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number',\n",
      "       'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
      "       'c_charge_degree', 'c_charge_desc', 'is_recid', 'r_case_number',\n",
      "       'r_charge_degree', 'r_days_from_arrest', 'r_offense_date',\n",
      "       'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid',\n",
      "       'is_violent_recid', 'vr_case_number', 'vr_charge_degree',\n",
      "       'vr_offense_date', 'vr_charge_desc', 'type_of_assessment',\n",
      "       'decile_score.1', 'score_text', 'screening_date',\n",
      "       'v_type_of_assessment', 'v_decile_score', 'v_score_text',\n",
      "       'v_screening_date', 'in_custody', 'out_custody', 'priors_count.1',\n",
      "       'start', 'end', 'event', 'two_year_recid'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>compas_screening_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>dob</th>\n",
       "      <th>age</th>\n",
       "      <th>age_cat</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>v_decile_score</th>\n",
       "      <th>v_score_text</th>\n",
       "      <th>v_screening_date</th>\n",
       "      <th>in_custody</th>\n",
       "      <th>out_custody</th>\n",
       "      <th>priors_count.1</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>event</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>miguel hernandez</td>\n",
       "      <td>miguel</td>\n",
       "      <td>hernandez</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1947-04-18</td>\n",
       "      <td>69</td>\n",
       "      <td>Greater than 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-08-14</td>\n",
       "      <td>2014-07-07</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>kevon dixon</td>\n",
       "      <td>kevon</td>\n",
       "      <td>dixon</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-01-22</td>\n",
       "      <td>34</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ed philo</td>\n",
       "      <td>ed</td>\n",
       "      <td>philo</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>Male</td>\n",
       "      <td>1991-05-14</td>\n",
       "      <td>24</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-04-14</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>marcu brown</td>\n",
       "      <td>marcu</td>\n",
       "      <td>brown</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>Male</td>\n",
       "      <td>1993-01-21</td>\n",
       "      <td>23</td>\n",
       "      <td>Less than 25</td>\n",
       "      <td>African-American</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>bouthy pierrelouis</td>\n",
       "      <td>bouthy</td>\n",
       "      <td>pierrelouis</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>Male</td>\n",
       "      <td>1973-01-22</td>\n",
       "      <td>43</td>\n",
       "      <td>25 - 45</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>2013-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                name   first         last compas_screening_date   sex  \\\n",
       "0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
       "1   3         kevon dixon   kevon        dixon            2013-01-27  Male   \n",
       "2   4            ed philo      ed        philo            2013-04-14  Male   \n",
       "3   5         marcu brown   marcu        brown            2013-01-13  Male   \n",
       "4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
       "\n",
       "          dob  age          age_cat              race  ...  v_decile_score  \\\n",
       "0  1947-04-18   69  Greater than 45             Other  ...               1   \n",
       "1  1982-01-22   34          25 - 45  African-American  ...               1   \n",
       "2  1991-05-14   24     Less than 25  African-American  ...               3   \n",
       "3  1993-01-21   23     Less than 25  African-American  ...               6   \n",
       "4  1973-01-22   43          25 - 45             Other  ...               1   \n",
       "\n",
       "   v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
       "0           Low        2013-08-14  2014-07-07   2014-07-14               0   \n",
       "1           Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
       "2           Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
       "3        Medium        2013-01-13         NaN          NaN               1   \n",
       "4           Low        2013-03-26         NaN          NaN               2   \n",
       "\n",
       "  start   end event two_year_recid  \n",
       "0     0   327     0              0  \n",
       "1     9   159     1              1  \n",
       "2     0    63     0              1  \n",
       "3     0  1174     0              0  \n",
       "4     0  1102     0              0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "raw_data = pd.read_csv('compas-scores-two-years.csv')\n",
    "# print a list of variable names\n",
    "print(raw_data.columns)\n",
    "# look at the first 5 rows \n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0cb9b-f285-4da2-8c5e-bd3af43ee4d7",
   "metadata": {},
   "source": [
    "The data set includes 53 variables. There are different types of information. Some variables\n",
    "* personal data (e.g., name, first name (\"first\"), last name (\"last\")) \n",
    "* demographic data (i.e., sex, age, age category (\"age_cat\"), and race)\n",
    "* related to the person's history of commited offenses (e.g., juvenile felony count (\"juv_fel_count\"), juvenile misdemeanor count (\"juv_misd_count\"), and prior offenses count (\"priors-count\"))\n",
    "* related to the charge against the person (e.g., charge offense date (\"c_offense_date\"), charge arrest date (\"c_arrest_date\"), charge degree (\"c_charge_degree\"), and description of charge (\"c_charge_desc\"))\n",
    "* recidivism scores assigned by the COMPAS algorithm (e.g., \"decile_score\", \"score_text\", \"v_decile_score\", \"v_score_text\")\n",
    "* related to an actual recidivism charge (e.g., degree of recidivism charge (\"r_charge_degree\"), data of recidivism offense (\"r_offense_date\"), description of recidivism charge (\"r_charge_desc\"))\n",
    "* related to an actual violent recidivism charge (e.g., degree of violent recidivism charge (\"vr_charge_degree\"), data of violent recidivism offense (\"vr_offense_date\"), description of violent recidivism charge (\"vr_charge_desc\")).\n",
    "\n",
    "### Part 1, Step 2: Select features and response variables\n",
    "\n",
    "The ProPublica article was assessing bias in the COMPAS scores. Here, you will ignore the COMPAS scores and instead explore the challenges of predicting recidivism based on the survey data. What variables seem like sensible predictors? What variables would be sensible outcome variables? The code in the cell below selects some numerical and categorical variables for you to include in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f2fcba38-1f99-4723-96d2-ddf46ac7f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and response variables\n",
    "\n",
    "# Features by type\n",
    "numerical_features = ['juv_misd_count', 'juv_other_count', 'juv_fel_count', \n",
    "    'priors_count', 'age']\n",
    "binary_categorical_features = ['sex', 'c_charge_degree']\n",
    "other_categorical_features = ['race']\n",
    "all_features = binary_categorical_features + other_categorical_features + numerical_features\n",
    "\n",
    "# Possible esponse variables\n",
    "response_variables = ['is_recid', 'is_violent_recid', 'two_year_recid']\n",
    "\n",
    "# Variables that are used for data cleaning\n",
    "check_variables = ['days_b_screening_arrest']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0cbaa-0a7b-46bf-a6d6-25f7c13fc1c7",
   "metadata": {},
   "source": [
    "ProPublica filtered some observations (i.e., rows in the data frame). See their explanation below. Let's follow their procedure.\n",
    "\n",
    "\n",
    "> There are a number of reasons remove rows because of missing data:\n",
    ">\n",
    "> * If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense.\n",
    "> * We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "> * In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "> * We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e11c07da-04e6-42be-b407-31569d56fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has 6172 rows and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "# Subselect data\n",
    "df = raw_data[all_features+response_variables+check_variables]\n",
    "\n",
    "# Apply filters\n",
    "df = df[(df['days_b_screening_arrest'] <= 30) & \n",
    "        (df['days_b_screening_arrest'] >= -30) & \n",
    "        (df['is_recid'] != -1) & \n",
    "        (df['c_charge_degree'] != 'O')]\n",
    "\n",
    "df = df[all_features+response_variables]\n",
    "print('Dataframe has {} rows and {} columns.'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24fa48-c751-4786-b0d6-54a25be38350",
   "metadata": {},
   "source": [
    "### Part 1, Step 3: Construct numerical coding for categorical features\n",
    "\n",
    "Some of these features in the subselected data are not numerical, so you will need to replace some categorical values with zeros and ones. Your features will include \"race\", which was surveyed as a one categorical variable with more than two categories. You will uses [1-hot encoding](https://en.wikipedia.org/wiki/One-hot) to include this feature in your data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a0fdd6d1-8ead-41ff-a9e6-6ce9149fc545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replace Female with 0.\n",
      "Replace Male with 1.\n",
      "Replace F with 0.\n",
      "Replace M with 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/1_fncc2d60bch9r3xcs1m_940000gn/T/ipykernel_47281/1287833026.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(value, new_value)\n"
     ]
    }
   ],
   "source": [
    "# Code binary features as 0 and 1\n",
    "for x in binary_categorical_features:\n",
    "    for new_value, value in enumerate(set(df[x])):\n",
    "        print(\"Replace {} with {}.\".format(value, new_value))\n",
    "        df = df.replace(value, new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f026e54b-d977-4ee3-93b8-d47703af9213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>race</th>\n",
       "      <th>race_is_Hispanic</th>\n",
       "      <th>race_is_Caucasian</th>\n",
       "      <th>race_is_Other</th>\n",
       "      <th>race_is_Native American</th>\n",
       "      <th>race_is_African-American</th>\n",
       "      <th>race_is_Asian</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>age</th>\n",
       "      <th>is_recid</th>\n",
       "      <th>is_violent_recid</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>African-American</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>African-American</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>African-American</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sex  c_charge_degree              race  race_is_Hispanic  \\\n",
       "0     1                0             Other             False   \n",
       "1     1                0  African-American             False   \n",
       "2     1                0  African-American             False   \n",
       "5     1                1             Other             False   \n",
       "6     1                0         Caucasian             False   \n",
       "7     1                0             Other             False   \n",
       "8     0                1         Caucasian             False   \n",
       "10    1                0         Caucasian             False   \n",
       "11    1                1  African-American             False   \n",
       "12    0                1         Caucasian             False   \n",
       "\n",
       "    race_is_Caucasian  race_is_Other  race_is_Native American  \\\n",
       "0               False           True                    False   \n",
       "1               False          False                    False   \n",
       "2               False          False                    False   \n",
       "5               False           True                    False   \n",
       "6                True          False                    False   \n",
       "7               False           True                    False   \n",
       "8                True          False                    False   \n",
       "10               True          False                    False   \n",
       "11              False          False                    False   \n",
       "12               True          False                    False   \n",
       "\n",
       "    race_is_African-American  race_is_Asian  juv_misd_count  juv_other_count  \\\n",
       "0                      False          False               0                0   \n",
       "1                       True          False               0                0   \n",
       "2                       True          False               0                1   \n",
       "5                      False          False               0                0   \n",
       "6                      False          False               0                0   \n",
       "7                      False          False               0                0   \n",
       "8                      False          False               0                0   \n",
       "10                     False          False               0                0   \n",
       "11                      True          False               0                0   \n",
       "12                     False          False               0                0   \n",
       "\n",
       "    juv_fel_count  priors_count  age  is_recid  is_violent_recid  \\\n",
       "0               0             0   69         0                 0   \n",
       "1               0             0   34         1                 1   \n",
       "2               0             4   24         1                 0   \n",
       "5               0             0   44         0                 0   \n",
       "6               0            14   41         1                 0   \n",
       "7               0             3   43         0                 0   \n",
       "8               0             0   39         0                 0   \n",
       "10              0             0   27         0                 0   \n",
       "11              0             3   23         1                 0   \n",
       "12              0             0   37         0                 0   \n",
       "\n",
       "    two_year_recid  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "5                0  \n",
       "6                1  \n",
       "7                0  \n",
       "8                0  \n",
       "10               0  \n",
       "11               1  \n",
       "12               0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 1-hot encoding for other categorical variables\n",
    "one_hot_features = []\n",
    "for x in other_categorical_features:\n",
    "    for new_feature, value in enumerate(set(df[x])):\n",
    "        feature_name = \"{}_is_{}\".format(x,value)\n",
    "        df.insert(3, feature_name, df[x]==value)\n",
    "        one_hot_features += [feature_name]\n",
    "\n",
    "# Check what the data frame looks like now\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b3c0b-4322-4ec6-80c8-da10665dc72b",
   "metadata": {},
   "source": [
    "### Part 1, Step 4: Split the data\n",
    "\n",
    "Let's collect the features in one data frame and the responses in another data frame. After that, you will set a small portion of the data set aside for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d9ea639f-feff-4bf2-ae40-4bfb6714cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features\n",
    "features = numerical_features + binary_categorical_features + one_hot_features\n",
    "\n",
    "# features data frame\n",
    "X = df[features]\n",
    "\n",
    "# responses data frame\n",
    "Y = df[response_variables]\n",
    "\n",
    "# Split the data into a training set containing 90% of the data\n",
    "# and test set containing 10% of the data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0556fa-cd30-439d-b671-80e165cfa5d2",
   "metadata": {},
   "source": [
    "# Part 2: Train and validate a decision tree\n",
    "\n",
    ">In this part, you will fit a decision tree to your data. You will examine the effect of tuning the complexity of the tree via the \"maximum number of leaves\" parameter and use 5-fold cross-validation to find an optimal value.\n",
    ">\n",
    ">This part includes three steps:\n",
    ">\n",
    ">1. Fit a decision tree on the training data\n",
    ">2. Tune the parameter \"maximum number of leaves\"\n",
    ">3. Calculate the selected model's test performance\n",
    "\n",
    "### Part 2, Step 1: Fit a decision tree on the training data\n",
    "\n",
    "Start by fitting a decision tree to your training data. Assess its training accuracy and its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "11c2b97d-2348-4e7b-b008-92b99665e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained decision tree with 1773 leaves and training accuracy 0.84.\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "# Fit model to training data\n",
    "dtc = dtc.fit(X_train, Y_train['two_year_recid'])    \n",
    "\n",
    "# Evaluate training accuracy\n",
    "prediction_values = dtc.predict(X_train)\n",
    "\n",
    "# average by counting \"1\"s (accurate results) and dividing by total number of observations\n",
    "count = 0\n",
    "for i in range(prediction_values.size):\n",
    "    if prediction_values[i] == Y_train['two_year_recid'].iloc[i]:\n",
    "        count = count + 1\n",
    "\n",
    "accuracy = count / prediction_values.size\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = dtc.get_n_leaves()\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and training accuracy {:.2f}.'.format(num_leaves, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e0628-df55-49f3-a00a-123ca44baa98",
   "metadata": {},
   "source": [
    "Your tree has a good training accuracy for the standards of tabular data prediction problems, but its size is enormous! It has so many leaves, that on average every 3 to 4 training observations get a leaf to themselves. It is very probable that this tree is overfitting.\n",
    "\n",
    "### Part 2, Step 2: Tune the parameter \"maximum number of leaves\"\n",
    "\n",
    "Let's try to constrain the complexity of a decision tree during training by setting a value for the argument ``maximum number of leaves``. You can use the sci-kit learn's `cross_val_score` function to quickly assess the out-of-sample performance of trees of varying complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e0668136-b649-4b42-be6b-e360ff7d2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaves\tMean accuracy\n",
      "---------------------\n",
      "0\t0.655\n",
      "1\t0.655\n",
      "2\t0.655\n",
      "3\t0.654\n",
      "4\t0.654\n",
      "5\t0.658\n",
      "6\t0.668\n",
      "7\t0.673\n",
      "8\t0.673\n",
      "9\t0.673\n",
      "10\t0.675\n",
      "11\t0.676\n",
      "12\t0.676\n",
      "13\t0.677\n",
      "14\t0.677\n",
      "15\t0.681\n",
      "16\t0.681\n",
      "17\t0.685\n",
      "18\t0.684\n",
      "19\t0.684\n",
      "20\t0.685\n",
      "21\t0.685\n",
      "22\t0.685\n",
      "23\t0.683\n",
      "24\t0.683\n",
      "25\t0.683\n",
      "26\t0.684\n",
      "27\t0.683\n",
      "28\t0.683\n",
      "29\t0.683\n",
      "30\t0.683\n",
      "31\t0.683\n",
      "32\t0.683\n",
      "33\t0.684\n",
      "34\t0.684\n",
      "35\t0.684\n",
      "36\t0.684\n",
      "37\t0.684\n",
      "38\t0.685\n",
      "39\t0.685\n",
      "40\t0.685\n",
      "41\t0.685\n",
      "42\t0.685\n",
      "43\t0.685\n",
      "44\t0.685\n",
      "45\t0.685\n",
      "46\t0.685\n",
      "47\t0.685\n",
      "48\t0.685\n",
      "49\t0.682\n",
      "50\t0.682\n",
      "51\t0.682\n",
      "52\t0.683\n",
      "53\t0.683\n",
      "54\t0.682\n",
      "55\t0.682\n",
      "56\t0.682\n",
      "57\t0.682\n",
      "58\t0.682\n",
      "59\t0.680\n",
      "60\t0.681\n",
      "61\t0.681\n",
      "62\t0.680\n",
      "63\t0.680\n",
      "64\t0.680\n",
      "65\t0.679\n",
      "66\t0.679\n",
      "67\t0.678\n",
      "68\t0.677\n",
      "69\t0.676\n",
      "70\t0.675\n",
      "71\t0.675\n",
      "72\t0.675\n",
      "73\t0.675\n",
      "74\t0.675\n",
      "75\t0.675\n",
      "76\t0.675\n",
      "77\t0.674\n",
      "78\t0.674\n",
      "79\t0.674\n",
      "80\t0.675\n",
      "81\t0.674\n",
      "82\t0.674\n",
      "83\t0.674\n",
      "84\t0.674\n",
      "85\t0.674\n",
      "86\t0.674\n",
      "87\t0.674\n",
      "88\t0.674\n",
      "89\t0.674\n",
      "90\t0.674\n",
      "91\t0.674\n",
      "92\t0.673\n",
      "93\t0.673\n",
      "94\t0.674\n",
      "95\t0.674\n",
      "96\t0.674\n",
      "97\t0.674\n",
      "98\t0.672\n",
      "99\t0.672\n",
      "100\t0.672\n",
      "101\t0.672\n",
      "102\t0.671\n",
      "103\t0.671\n",
      "104\t0.670\n",
      "105\t0.670\n",
      "106\t0.670\n",
      "107\t0.670\n",
      "108\t0.670\n",
      "109\t0.670\n",
      "110\t0.670\n",
      "111\t0.670\n",
      "112\t0.670\n",
      "113\t0.670\n",
      "114\t0.670\n",
      "115\t0.670\n",
      "116\t0.670\n",
      "117\t0.670\n",
      "118\t0.669\n",
      "119\t0.669\n",
      "120\t0.669\n",
      "121\t0.669\n",
      "122\t0.669\n",
      "123\t0.670\n",
      "124\t0.669\n",
      "125\t0.669\n",
      "126\t0.669\n",
      "127\t0.668\n",
      "128\t0.668\n",
      "129\t0.669\n",
      "130\t0.669\n",
      "131\t0.668\n",
      "132\t0.667\n",
      "133\t0.668\n",
      "134\t0.668\n",
      "135\t0.668\n",
      "136\t0.668\n",
      "137\t0.668\n",
      "138\t0.668\n",
      "139\t0.668\n",
      "140\t0.668\n",
      "141\t0.667\n",
      "142\t0.668\n",
      "143\t0.667\n",
      "144\t0.667\n",
      "145\t0.667\n",
      "146\t0.666\n",
      "147\t0.665\n",
      "148\t0.666\n",
      "149\t0.664\n",
      "150\t0.664\n",
      "151\t0.664\n",
      "152\t0.664\n",
      "153\t0.664\n",
      "154\t0.665\n",
      "155\t0.665\n",
      "156\t0.665\n",
      "157\t0.664\n",
      "158\t0.665\n",
      "159\t0.665\n",
      "160\t0.665\n",
      "161\t0.665\n",
      "162\t0.663\n",
      "163\t0.663\n",
      "164\t0.663\n",
      "165\t0.662\n",
      "166\t0.662\n",
      "167\t0.662\n",
      "168\t0.662\n",
      "169\t0.662\n",
      "170\t0.662\n",
      "171\t0.661\n",
      "172\t0.662\n",
      "173\t0.662\n",
      "174\t0.662\n",
      "175\t0.662\n",
      "176\t0.662\n",
      "177\t0.662\n",
      "178\t0.662\n",
      "179\t0.662\n",
      "180\t0.662\n",
      "181\t0.662\n",
      "182\t0.662\n",
      "183\t0.661\n",
      "184\t0.661\n",
      "185\t0.661\n",
      "186\t0.662\n",
      "187\t0.662\n",
      "188\t0.662\n",
      "189\t0.662\n",
      "190\t0.662\n",
      "191\t0.662\n",
      "192\t0.662\n",
      "193\t0.662\n",
      "194\t0.661\n",
      "195\t0.661\n",
      "196\t0.661\n",
      "197\t0.661\n",
      "198\t0.661\n",
      "199\t0.661\n",
      "200\t0.661\n",
      "201\t0.661\n",
      "202\t0.660\n",
      "203\t0.660\n",
      "204\t0.660\n",
      "205\t0.659\n",
      "206\t0.659\n",
      "207\t0.658\n",
      "208\t0.658\n",
      "209\t0.658\n",
      "210\t0.658\n",
      "211\t0.658\n",
      "212\t0.658\n",
      "213\t0.659\n",
      "214\t0.659\n",
      "215\t0.659\n",
      "216\t0.659\n",
      "217\t0.658\n",
      "218\t0.658\n",
      "219\t0.658\n",
      "220\t0.658\n",
      "221\t0.658\n",
      "222\t0.658\n",
      "223\t0.658\n",
      "224\t0.658\n",
      "225\t0.659\n",
      "226\t0.658\n",
      "227\t0.658\n",
      "228\t0.658\n",
      "229\t0.658\n",
      "230\t0.658\n",
      "231\t0.657\n",
      "232\t0.657\n",
      "233\t0.657\n",
      "234\t0.658\n",
      "235\t0.658\n",
      "236\t0.658\n",
      "237\t0.658\n",
      "238\t0.658\n",
      "239\t0.658\n",
      "240\t0.658\n",
      "241\t0.657\n",
      "242\t0.657\n",
      "243\t0.657\n",
      "244\t0.657\n",
      "245\t0.657\n",
      "246\t0.658\n",
      "247\t0.658\n",
      "248\t0.658\n",
      "249\t0.658\n",
      "250\t0.657\n",
      "251\t0.657\n",
      "252\t0.657\n",
      "253\t0.657\n",
      "254\t0.657\n",
      "255\t0.657\n",
      "256\t0.656\n",
      "257\t0.656\n",
      "258\t0.656\n",
      "259\t0.655\n",
      "260\t0.655\n",
      "261\t0.656\n",
      "262\t0.656\n",
      "263\t0.656\n",
      "264\t0.656\n",
      "265\t0.656\n",
      "266\t0.655\n",
      "267\t0.656\n",
      "268\t0.657\n",
      "269\t0.656\n",
      "270\t0.656\n",
      "271\t0.656\n",
      "272\t0.656\n",
      "273\t0.657\n",
      "274\t0.657\n",
      "275\t0.656\n",
      "276\t0.656\n",
      "277\t0.656\n",
      "278\t0.656\n",
      "279\t0.656\n",
      "280\t0.656\n",
      "281\t0.656\n",
      "282\t0.656\n",
      "283\t0.655\n",
      "284\t0.656\n",
      "285\t0.656\n",
      "286\t0.656\n",
      "287\t0.655\n",
      "288\t0.655\n",
      "289\t0.655\n",
      "290\t0.655\n",
      "291\t0.655\n",
      "292\t0.655\n",
      "293\t0.654\n",
      "294\t0.654\n",
      "295\t0.654\n",
      "296\t0.654\n",
      "297\t0.654\n",
      "298\t0.654\n",
      "299\t0.654\n",
      "300\t0.653\n",
      "301\t0.653\n",
      "302\t0.653\n",
      "303\t0.653\n",
      "304\t0.653\n",
      "305\t0.653\n",
      "306\t0.651\n",
      "307\t0.651\n",
      "308\t0.652\n",
      "309\t0.652\n",
      "310\t0.651\n",
      "311\t0.652\n",
      "312\t0.652\n",
      "313\t0.652\n",
      "314\t0.652\n",
      "315\t0.651\n",
      "316\t0.652\n",
      "317\t0.652\n",
      "318\t0.651\n",
      "319\t0.651\n",
      "320\t0.651\n",
      "321\t0.651\n",
      "322\t0.651\n",
      "323\t0.651\n",
      "324\t0.649\n",
      "325\t0.649\n",
      "326\t0.649\n",
      "327\t0.648\n",
      "328\t0.648\n",
      "329\t0.648\n",
      "330\t0.648\n",
      "331\t0.648\n",
      "332\t0.649\n",
      "333\t0.649\n",
      "334\t0.648\n",
      "335\t0.649\n",
      "336\t0.649\n",
      "337\t0.649\n",
      "338\t0.648\n",
      "339\t0.647\n",
      "340\t0.647\n",
      "341\t0.647\n",
      "342\t0.647\n",
      "343\t0.646\n",
      "344\t0.646\n",
      "345\t0.646\n",
      "346\t0.647\n",
      "347\t0.646\n",
      "348\t0.646\n",
      "349\t0.646\n",
      "350\t0.646\n",
      "351\t0.646\n",
      "352\t0.646\n",
      "353\t0.646\n",
      "354\t0.646\n",
      "355\t0.646\n",
      "356\t0.646\n",
      "357\t0.646\n",
      "358\t0.645\n",
      "359\t0.645\n",
      "360\t0.644\n",
      "361\t0.644\n",
      "362\t0.644\n",
      "363\t0.644\n",
      "364\t0.643\n",
      "365\t0.644\n",
      "366\t0.643\n",
      "367\t0.643\n",
      "368\t0.642\n",
      "369\t0.643\n",
      "370\t0.643\n",
      "371\t0.642\n",
      "372\t0.642\n",
      "373\t0.642\n",
      "374\t0.641\n",
      "375\t0.641\n",
      "376\t0.641\n",
      "377\t0.642\n",
      "378\t0.642\n",
      "379\t0.644\n",
      "380\t0.644\n",
      "381\t0.643\n",
      "382\t0.643\n",
      "383\t0.643\n",
      "384\t0.643\n",
      "385\t0.643\n",
      "386\t0.643\n",
      "387\t0.643\n",
      "388\t0.643\n",
      "389\t0.644\n",
      "390\t0.643\n",
      "391\t0.643\n",
      "392\t0.643\n",
      "393\t0.643\n",
      "394\t0.643\n",
      "395\t0.642\n",
      "396\t0.642\n",
      "397\t0.643\n",
      "398\t0.643\n",
      "399\t0.643\n",
      "400\t0.643\n",
      "401\t0.643\n",
      "402\t0.643\n",
      "403\t0.642\n",
      "404\t0.643\n",
      "405\t0.642\n",
      "406\t0.642\n",
      "407\t0.642\n",
      "408\t0.642\n",
      "409\t0.642\n",
      "410\t0.641\n",
      "411\t0.642\n",
      "412\t0.642\n",
      "413\t0.641\n",
      "414\t0.642\n",
      "415\t0.642\n",
      "416\t0.641\n",
      "417\t0.641\n",
      "418\t0.641\n",
      "419\t0.641\n",
      "420\t0.640\n",
      "421\t0.640\n",
      "422\t0.640\n",
      "423\t0.640\n",
      "424\t0.641\n",
      "425\t0.640\n",
      "426\t0.640\n",
      "427\t0.640\n",
      "428\t0.640\n",
      "429\t0.639\n",
      "430\t0.638\n",
      "431\t0.638\n",
      "432\t0.638\n",
      "433\t0.638\n",
      "434\t0.639\n",
      "435\t0.639\n",
      "436\t0.639\n",
      "437\t0.639\n",
      "438\t0.639\n",
      "439\t0.639\n",
      "440\t0.639\n",
      "441\t0.639\n",
      "442\t0.639\n",
      "443\t0.639\n",
      "444\t0.639\n",
      "445\t0.639\n",
      "446\t0.639\n",
      "447\t0.639\n",
      "448\t0.639\n",
      "449\t0.639\n",
      "450\t0.639\n",
      "451\t0.639\n",
      "452\t0.639\n",
      "453\t0.639\n",
      "454\t0.639\n",
      "455\t0.639\n",
      "456\t0.639\n",
      "457\t0.639\n",
      "458\t0.640\n",
      "459\t0.640\n",
      "460\t0.639\n",
      "461\t0.638\n",
      "462\t0.638\n",
      "463\t0.638\n",
      "464\t0.638\n",
      "465\t0.638\n",
      "466\t0.638\n",
      "467\t0.638\n",
      "468\t0.638\n",
      "469\t0.638\n",
      "470\t0.638\n",
      "471\t0.638\n",
      "472\t0.638\n",
      "473\t0.639\n",
      "474\t0.638\n",
      "475\t0.638\n",
      "476\t0.638\n",
      "477\t0.638\n",
      "478\t0.638\n",
      "479\t0.638\n",
      "480\t0.638\n",
      "481\t0.638\n",
      "482\t0.638\n",
      "483\t0.638\n",
      "484\t0.637\n",
      "485\t0.637\n",
      "486\t0.637\n",
      "487\t0.637\n",
      "488\t0.637\n",
      "489\t0.637\n",
      "490\t0.637\n",
      "491\t0.637\n",
      "492\t0.637\n",
      "493\t0.637\n",
      "494\t0.638\n",
      "495\t0.638\n",
      "496\t0.638\n",
      "497\t0.638\n",
      "498\t0.638\n",
      "499\t0.638\n",
      "500\t0.638\n",
      "501\t0.638\n",
      "502\t0.637\n",
      "503\t0.637\n",
      "504\t0.637\n",
      "505\t0.637\n",
      "506\t0.637\n",
      "507\t0.637\n",
      "508\t0.637\n",
      "509\t0.637\n",
      "510\t0.637\n",
      "511\t0.637\n",
      "512\t0.637\n",
      "513\t0.637\n",
      "514\t0.637\n",
      "515\t0.637\n",
      "516\t0.636\n",
      "517\t0.636\n",
      "518\t0.636\n",
      "519\t0.636\n",
      "520\t0.636\n",
      "521\t0.636\n",
      "522\t0.636\n",
      "523\t0.635\n",
      "524\t0.635\n",
      "525\t0.635\n",
      "526\t0.635\n",
      "527\t0.634\n",
      "528\t0.635\n",
      "529\t0.634\n",
      "530\t0.635\n",
      "531\t0.635\n",
      "532\t0.635\n",
      "533\t0.636\n",
      "534\t0.635\n",
      "535\t0.635\n",
      "536\t0.635\n",
      "537\t0.635\n",
      "538\t0.636\n",
      "539\t0.635\n",
      "540\t0.635\n",
      "541\t0.635\n",
      "542\t0.635\n",
      "543\t0.635\n",
      "544\t0.634\n",
      "545\t0.634\n",
      "546\t0.634\n",
      "547\t0.634\n",
      "548\t0.635\n",
      "549\t0.635\n",
      "550\t0.634\n",
      "551\t0.634\n",
      "552\t0.634\n",
      "553\t0.634\n",
      "554\t0.634\n",
      "555\t0.634\n",
      "556\t0.634\n",
      "557\t0.634\n",
      "558\t0.633\n",
      "559\t0.633\n",
      "560\t0.633\n",
      "561\t0.633\n",
      "562\t0.633\n",
      "563\t0.633\n",
      "564\t0.633\n",
      "565\t0.633\n",
      "566\t0.633\n",
      "567\t0.633\n",
      "568\t0.633\n",
      "569\t0.633\n",
      "570\t0.633\n",
      "571\t0.633\n",
      "572\t0.633\n",
      "573\t0.633\n",
      "574\t0.633\n",
      "575\t0.633\n",
      "576\t0.632\n",
      "577\t0.632\n",
      "578\t0.632\n",
      "579\t0.632\n",
      "580\t0.632\n",
      "581\t0.631\n",
      "582\t0.631\n",
      "583\t0.631\n",
      "584\t0.631\n",
      "585\t0.631\n",
      "586\t0.631\n",
      "587\t0.631\n",
      "588\t0.631\n",
      "589\t0.631\n",
      "590\t0.631\n",
      "591\t0.630\n",
      "592\t0.630\n",
      "593\t0.630\n",
      "594\t0.630\n",
      "595\t0.630\n",
      "596\t0.630\n",
      "597\t0.630\n",
      "598\t0.630\n",
      "599\t0.630\n",
      "600\t0.630\n",
      "601\t0.630\n",
      "602\t0.630\n",
      "603\t0.630\n",
      "604\t0.630\n",
      "605\t0.630\n",
      "606\t0.630\n",
      "607\t0.630\n",
      "608\t0.630\n",
      "609\t0.630\n",
      "610\t0.630\n",
      "611\t0.630\n",
      "612\t0.630\n",
      "613\t0.629\n",
      "614\t0.630\n",
      "615\t0.630\n",
      "616\t0.630\n",
      "617\t0.629\n",
      "618\t0.629\n",
      "619\t0.629\n",
      "620\t0.629\n",
      "621\t0.629\n",
      "622\t0.629\n",
      "623\t0.629\n",
      "624\t0.629\n",
      "625\t0.629\n",
      "626\t0.628\n",
      "627\t0.628\n",
      "628\t0.628\n",
      "629\t0.628\n",
      "630\t0.628\n",
      "631\t0.628\n",
      "632\t0.628\n",
      "633\t0.628\n",
      "634\t0.628\n",
      "635\t0.628\n",
      "636\t0.628\n",
      "637\t0.628\n",
      "638\t0.628\n",
      "639\t0.628\n",
      "640\t0.628\n",
      "641\t0.628\n",
      "642\t0.628\n",
      "643\t0.627\n",
      "644\t0.628\n",
      "645\t0.628\n",
      "646\t0.628\n",
      "647\t0.627\n",
      "648\t0.628\n",
      "649\t0.627\n",
      "650\t0.627\n",
      "651\t0.627\n",
      "652\t0.627\n",
      "653\t0.627\n",
      "654\t0.627\n",
      "655\t0.627\n",
      "656\t0.627\n",
      "657\t0.627\n",
      "658\t0.627\n",
      "659\t0.627\n",
      "660\t0.627\n",
      "661\t0.627\n",
      "662\t0.627\n",
      "663\t0.627\n",
      "664\t0.627\n",
      "665\t0.626\n",
      "666\t0.626\n",
      "667\t0.626\n",
      "668\t0.626\n",
      "669\t0.626\n",
      "670\t0.626\n",
      "671\t0.626\n",
      "672\t0.626\n",
      "673\t0.626\n",
      "674\t0.626\n",
      "675\t0.626\n",
      "676\t0.626\n",
      "677\t0.626\n",
      "678\t0.626\n",
      "679\t0.626\n",
      "680\t0.626\n",
      "681\t0.626\n",
      "682\t0.626\n",
      "683\t0.626\n",
      "684\t0.626\n",
      "685\t0.626\n",
      "686\t0.625\n",
      "687\t0.625\n",
      "688\t0.625\n",
      "689\t0.625\n",
      "690\t0.625\n",
      "691\t0.625\n",
      "692\t0.625\n",
      "693\t0.625\n",
      "694\t0.625\n",
      "695\t0.625\n",
      "696\t0.625\n",
      "697\t0.625\n",
      "698\t0.625\n",
      "699\t0.625\n",
      "700\t0.625\n",
      "701\t0.625\n",
      "702\t0.625\n",
      "703\t0.625\n",
      "704\t0.625\n",
      "705\t0.625\n",
      "706\t0.625\n",
      "707\t0.625\n",
      "708\t0.625\n",
      "709\t0.624\n",
      "710\t0.624\n",
      "711\t0.625\n",
      "712\t0.624\n",
      "713\t0.624\n",
      "714\t0.624\n",
      "715\t0.624\n",
      "716\t0.624\n",
      "717\t0.624\n",
      "718\t0.624\n",
      "719\t0.623\n",
      "720\t0.623\n",
      "721\t0.624\n",
      "722\t0.624\n",
      "723\t0.624\n",
      "724\t0.624\n",
      "725\t0.624\n",
      "726\t0.624\n",
      "727\t0.624\n",
      "728\t0.624\n",
      "729\t0.624\n",
      "730\t0.624\n",
      "731\t0.623\n",
      "732\t0.623\n",
      "733\t0.623\n",
      "734\t0.623\n",
      "735\t0.623\n",
      "736\t0.623\n",
      "737\t0.623\n",
      "738\t0.623\n",
      "739\t0.623\n",
      "740\t0.623\n",
      "741\t0.623\n",
      "742\t0.623\n",
      "743\t0.623\n",
      "744\t0.624\n",
      "745\t0.623\n",
      "746\t0.623\n",
      "747\t0.623\n",
      "748\t0.623\n",
      "749\t0.623\n",
      "750\t0.623\n",
      "751\t0.622\n",
      "752\t0.622\n",
      "753\t0.622\n",
      "754\t0.623\n",
      "755\t0.622\n",
      "756\t0.622\n",
      "757\t0.622\n",
      "758\t0.623\n",
      "759\t0.623\n",
      "760\t0.623\n",
      "761\t0.623\n",
      "762\t0.622\n",
      "763\t0.622\n",
      "764\t0.622\n",
      "765\t0.622\n",
      "766\t0.622\n",
      "767\t0.622\n",
      "768\t0.622\n",
      "769\t0.622\n",
      "770\t0.622\n",
      "771\t0.622\n",
      "772\t0.622\n",
      "773\t0.622\n",
      "774\t0.622\n",
      "775\t0.622\n",
      "776\t0.622\n",
      "777\t0.621\n",
      "778\t0.621\n",
      "779\t0.621\n",
      "780\t0.621\n",
      "781\t0.621\n",
      "782\t0.621\n",
      "783\t0.621\n",
      "784\t0.621\n",
      "785\t0.621\n",
      "786\t0.621\n",
      "787\t0.621\n",
      "788\t0.621\n",
      "789\t0.620\n",
      "790\t0.621\n",
      "791\t0.621\n",
      "792\t0.621\n",
      "793\t0.621\n",
      "794\t0.621\n",
      "795\t0.621\n",
      "796\t0.621\n",
      "797\t0.621\n",
      "798\t0.621\n",
      "799\t0.621\n",
      "800\t0.620\n",
      "801\t0.620\n",
      "802\t0.620\n",
      "803\t0.620\n",
      "804\t0.620\n",
      "805\t0.620\n",
      "806\t0.620\n",
      "807\t0.619\n",
      "808\t0.620\n",
      "809\t0.619\n",
      "810\t0.619\n",
      "811\t0.619\n",
      "812\t0.619\n",
      "813\t0.619\n",
      "814\t0.619\n",
      "815\t0.618\n",
      "816\t0.618\n",
      "817\t0.618\n",
      "818\t0.618\n",
      "819\t0.618\n",
      "820\t0.618\n",
      "821\t0.618\n",
      "822\t0.618\n",
      "823\t0.618\n",
      "824\t0.618\n",
      "825\t0.618\n",
      "826\t0.618\n",
      "827\t0.618\n",
      "828\t0.618\n",
      "829\t0.618\n",
      "830\t0.618\n",
      "831\t0.618\n",
      "832\t0.618\n",
      "833\t0.618\n",
      "834\t0.617\n",
      "835\t0.618\n",
      "836\t0.617\n",
      "837\t0.617\n",
      "838\t0.617\n",
      "839\t0.617\n",
      "840\t0.617\n",
      "841\t0.617\n",
      "842\t0.617\n",
      "843\t0.617\n",
      "844\t0.617\n",
      "845\t0.617\n",
      "846\t0.617\n",
      "847\t0.617\n",
      "848\t0.617\n",
      "849\t0.616\n",
      "850\t0.616\n",
      "851\t0.616\n",
      "852\t0.616\n",
      "853\t0.616\n",
      "854\t0.616\n",
      "855\t0.616\n",
      "856\t0.616\n",
      "857\t0.616\n",
      "858\t0.616\n",
      "859\t0.616\n",
      "860\t0.616\n",
      "861\t0.616\n",
      "862\t0.616\n",
      "863\t0.616\n",
      "864\t0.616\n",
      "865\t0.615\n",
      "866\t0.615\n",
      "867\t0.615\n",
      "868\t0.615\n",
      "869\t0.615\n",
      "870\t0.615\n",
      "871\t0.615\n",
      "872\t0.615\n",
      "873\t0.615\n",
      "874\t0.615\n",
      "875\t0.615\n",
      "876\t0.615\n",
      "877\t0.615\n",
      "878\t0.615\n",
      "879\t0.615\n",
      "880\t0.615\n",
      "881\t0.615\n",
      "882\t0.615\n",
      "883\t0.615\n",
      "884\t0.615\n",
      "885\t0.615\n",
      "886\t0.615\n",
      "887\t0.615\n",
      "888\t0.615\n",
      "889\t0.615\n",
      "890\t0.615\n",
      "891\t0.615\n",
      "892\t0.615\n",
      "893\t0.615\n",
      "894\t0.615\n",
      "895\t0.615\n",
      "896\t0.615\n",
      "897\t0.614\n",
      "898\t0.614\n",
      "899\t0.614\n",
      "900\t0.615\n",
      "901\t0.615\n",
      "902\t0.614\n",
      "903\t0.614\n",
      "904\t0.614\n",
      "905\t0.614\n",
      "906\t0.614\n",
      "907\t0.614\n",
      "908\t0.614\n",
      "909\t0.614\n",
      "910\t0.614\n",
      "911\t0.614\n",
      "912\t0.614\n",
      "913\t0.614\n",
      "914\t0.614\n",
      "915\t0.614\n",
      "916\t0.614\n",
      "917\t0.614\n",
      "918\t0.614\n",
      "919\t0.614\n",
      "920\t0.614\n",
      "921\t0.614\n",
      "922\t0.613\n",
      "923\t0.613\n",
      "924\t0.613\n",
      "925\t0.613\n",
      "926\t0.613\n",
      "927\t0.614\n",
      "928\t0.613\n",
      "929\t0.613\n",
      "930\t0.613\n",
      "931\t0.613\n",
      "932\t0.613\n",
      "933\t0.613\n",
      "934\t0.613\n",
      "935\t0.613\n",
      "936\t0.613\n",
      "937\t0.613\n",
      "938\t0.613\n",
      "939\t0.613\n",
      "940\t0.613\n",
      "941\t0.613\n",
      "942\t0.613\n",
      "943\t0.613\n",
      "944\t0.612\n",
      "945\t0.612\n",
      "946\t0.612\n",
      "947\t0.612\n",
      "948\t0.612\n",
      "949\t0.612\n",
      "950\t0.612\n",
      "951\t0.612\n",
      "952\t0.612\n",
      "953\t0.612\n",
      "954\t0.611\n",
      "955\t0.611\n",
      "956\t0.611\n",
      "957\t0.612\n",
      "958\t0.612\n",
      "959\t0.612\n",
      "960\t0.612\n",
      "961\t0.612\n",
      "962\t0.612\n",
      "963\t0.612\n",
      "964\t0.612\n",
      "965\t0.612\n",
      "966\t0.612\n",
      "967\t0.612\n",
      "968\t0.612\n",
      "969\t0.612\n",
      "970\t0.612\n",
      "971\t0.612\n",
      "972\t0.612\n",
      "973\t0.612\n",
      "974\t0.612\n",
      "975\t0.612\n",
      "976\t0.612\n",
      "977\t0.613\n",
      "978\t0.613\n",
      "979\t0.612\n",
      "980\t0.612\n",
      "981\t0.612\n",
      "982\t0.612\n",
      "983\t0.612\n",
      "984\t0.612\n",
      "985\t0.612\n",
      "986\t0.613\n",
      "987\t0.612\n",
      "988\t0.612\n",
      "989\t0.612\n",
      "990\t0.612\n",
      "991\t0.612\n",
      "992\t0.612\n",
      "993\t0.612\n",
      "994\t0.612\n",
      "995\t0.612\n",
      "996\t0.612\n",
      "997\t0.613\n",
      "998\t0.612\n",
      "999\t0.612\n",
      "1000\t0.612\n",
      "1001\t0.612\n",
      "1002\t0.612\n",
      "1003\t0.612\n",
      "1004\t0.612\n",
      "1005\t0.612\n",
      "1006\t0.612\n",
      "1007\t0.612\n",
      "1008\t0.612\n",
      "1009\t0.612\n",
      "1010\t0.612\n",
      "1011\t0.612\n",
      "1012\t0.612\n",
      "1013\t0.612\n",
      "1014\t0.612\n",
      "1015\t0.612\n",
      "1016\t0.612\n",
      "1017\t0.612\n",
      "1018\t0.612\n",
      "1019\t0.612\n",
      "1020\t0.612\n",
      "1021\t0.611\n",
      "1022\t0.611\n",
      "1023\t0.611\n",
      "1024\t0.611\n",
      "1025\t0.611\n",
      "1026\t0.611\n",
      "1027\t0.611\n",
      "1028\t0.611\n",
      "1029\t0.611\n",
      "1030\t0.611\n",
      "1031\t0.611\n",
      "1032\t0.611\n",
      "1033\t0.611\n",
      "1034\t0.611\n",
      "1035\t0.611\n",
      "1036\t0.611\n",
      "1037\t0.611\n",
      "1038\t0.611\n",
      "1039\t0.611\n",
      "1040\t0.611\n",
      "1041\t0.611\n",
      "1042\t0.611\n",
      "1043\t0.611\n",
      "1044\t0.611\n",
      "1045\t0.611\n",
      "1046\t0.611\n",
      "1047\t0.611\n",
      "1048\t0.611\n",
      "1049\t0.611\n",
      "1050\t0.611\n",
      "1051\t0.612\n",
      "1052\t0.612\n",
      "1053\t0.612\n",
      "1054\t0.612\n",
      "1055\t0.612\n",
      "1056\t0.612\n",
      "1057\t0.612\n",
      "1058\t0.612\n",
      "1059\t0.612\n",
      "1060\t0.612\n",
      "1061\t0.612\n",
      "1062\t0.612\n",
      "1063\t0.612\n",
      "1064\t0.612\n",
      "1065\t0.612\n",
      "1066\t0.612\n",
      "1067\t0.612\n",
      "1068\t0.612\n",
      "1069\t0.612\n",
      "1070\t0.612\n",
      "1071\t0.612\n",
      "1072\t0.612\n",
      "1073\t0.612\n",
      "1074\t0.612\n",
      "1075\t0.612\n",
      "1076\t0.612\n",
      "1077\t0.611\n",
      "1078\t0.611\n",
      "1079\t0.611\n",
      "1080\t0.611\n",
      "1081\t0.611\n",
      "1082\t0.611\n",
      "1083\t0.611\n",
      "1084\t0.611\n",
      "1085\t0.611\n",
      "1086\t0.611\n",
      "1087\t0.611\n",
      "1088\t0.611\n",
      "1089\t0.611\n",
      "1090\t0.611\n",
      "1091\t0.611\n",
      "1092\t0.611\n",
      "1093\t0.611\n",
      "1094\t0.611\n",
      "1095\t0.611\n",
      "1096\t0.611\n",
      "1097\t0.611\n",
      "1098\t0.611\n",
      "1099\t0.611\n",
      "1100\t0.611\n",
      "1101\t0.611\n",
      "1102\t0.611\n",
      "1103\t0.611\n",
      "1104\t0.611\n",
      "1105\t0.611\n",
      "1106\t0.611\n",
      "1107\t0.612\n",
      "1108\t0.612\n",
      "1109\t0.612\n",
      "1110\t0.612\n",
      "1111\t0.612\n",
      "1112\t0.612\n",
      "1113\t0.612\n",
      "1114\t0.612\n",
      "1115\t0.612\n",
      "1116\t0.612\n",
      "1117\t0.612\n",
      "1118\t0.612\n",
      "1119\t0.612\n",
      "1120\t0.612\n",
      "1121\t0.612\n",
      "1122\t0.612\n",
      "1123\t0.612\n",
      "1124\t0.612\n",
      "1125\t0.612\n",
      "1126\t0.612\n",
      "1127\t0.612\n",
      "1128\t0.611\n",
      "1129\t0.611\n",
      "1130\t0.612\n",
      "1131\t0.611\n",
      "1132\t0.611\n",
      "1133\t0.611\n",
      "1134\t0.611\n",
      "1135\t0.611\n",
      "1136\t0.611\n",
      "1137\t0.611\n",
      "1138\t0.611\n",
      "1139\t0.611\n",
      "1140\t0.611\n",
      "1141\t0.611\n",
      "1142\t0.611\n",
      "1143\t0.611\n",
      "1144\t0.611\n",
      "1145\t0.611\n",
      "1146\t0.611\n",
      "1147\t0.611\n",
      "1148\t0.611\n",
      "1149\t0.611\n",
      "1150\t0.611\n",
      "1151\t0.611\n",
      "1152\t0.611\n",
      "1153\t0.611\n",
      "1154\t0.611\n",
      "1155\t0.611\n",
      "1156\t0.611\n",
      "1157\t0.611\n",
      "1158\t0.611\n",
      "1159\t0.610\n",
      "1160\t0.611\n",
      "1161\t0.611\n",
      "1162\t0.609\n",
      "1163\t0.609\n",
      "1164\t0.609\n",
      "1165\t0.609\n",
      "1166\t0.609\n",
      "1167\t0.609\n",
      "1168\t0.609\n",
      "1169\t0.609\n",
      "1170\t0.609\n",
      "1171\t0.609\n",
      "1172\t0.609\n",
      "1173\t0.609\n",
      "1174\t0.609\n",
      "1175\t0.609\n",
      "1176\t0.609\n",
      "1177\t0.609\n",
      "1178\t0.609\n",
      "1179\t0.609\n",
      "1180\t0.609\n",
      "1181\t0.609\n",
      "1182\t0.609\n",
      "1183\t0.609\n",
      "1184\t0.609\n",
      "1185\t0.609\n",
      "1186\t0.609\n",
      "1187\t0.609\n",
      "1188\t0.609\n",
      "1189\t0.609\n",
      "1190\t0.609\n",
      "1191\t0.609\n",
      "1192\t0.609\n",
      "1193\t0.609\n",
      "1194\t0.609\n",
      "1195\t0.609\n",
      "1196\t0.609\n",
      "1197\t0.609\n",
      "1198\t0.609\n",
      "1199\t0.609\n",
      "1200\t0.609\n",
      "1201\t0.609\n",
      "1202\t0.609\n",
      "1203\t0.609\n",
      "1204\t0.609\n",
      "1205\t0.609\n",
      "1206\t0.609\n",
      "1207\t0.609\n",
      "1208\t0.609\n",
      "1209\t0.609\n",
      "1210\t0.609\n",
      "1211\t0.609\n",
      "1212\t0.609\n",
      "1213\t0.609\n",
      "1214\t0.609\n",
      "1215\t0.609\n",
      "1216\t0.609\n",
      "1217\t0.609\n",
      "1218\t0.609\n",
      "1219\t0.609\n",
      "1220\t0.609\n",
      "1221\t0.609\n",
      "1222\t0.609\n",
      "1223\t0.610\n",
      "1224\t0.609\n",
      "1225\t0.609\n",
      "1226\t0.609\n",
      "1227\t0.609\n",
      "1228\t0.609\n",
      "1229\t0.609\n",
      "1230\t0.609\n",
      "1231\t0.609\n",
      "1232\t0.609\n",
      "1233\t0.609\n",
      "1234\t0.608\n",
      "1235\t0.608\n",
      "1236\t0.608\n",
      "1237\t0.608\n",
      "1238\t0.608\n",
      "1239\t0.608\n",
      "1240\t0.608\n",
      "1241\t0.608\n",
      "1242\t0.608\n",
      "1243\t0.608\n",
      "1244\t0.607\n",
      "1245\t0.607\n",
      "1246\t0.608\n",
      "1247\t0.608\n",
      "1248\t0.608\n",
      "1249\t0.608\n",
      "1250\t0.608\n",
      "1251\t0.608\n",
      "1252\t0.608\n",
      "1253\t0.608\n",
      "1254\t0.608\n",
      "1255\t0.608\n",
      "1256\t0.608\n",
      "1257\t0.608\n",
      "1258\t0.608\n",
      "1259\t0.608\n",
      "1260\t0.608\n",
      "1261\t0.608\n",
      "1262\t0.607\n",
      "1263\t0.607\n",
      "1264\t0.607\n",
      "1265\t0.607\n",
      "1266\t0.607\n",
      "1267\t0.607\n",
      "1268\t0.607\n",
      "1269\t0.607\n",
      "1270\t0.607\n",
      "1271\t0.607\n",
      "1272\t0.607\n",
      "1273\t0.607\n",
      "1274\t0.607\n",
      "1275\t0.607\n",
      "1276\t0.607\n",
      "1277\t0.607\n",
      "1278\t0.607\n",
      "1279\t0.607\n",
      "1280\t0.607\n",
      "1281\t0.607\n",
      "1282\t0.607\n",
      "1283\t0.607\n",
      "1284\t0.607\n",
      "1285\t0.607\n",
      "1286\t0.607\n",
      "1287\t0.607\n",
      "1288\t0.607\n",
      "1289\t0.607\n",
      "1290\t0.607\n",
      "1291\t0.607\n",
      "1292\t0.607\n",
      "1293\t0.607\n",
      "1294\t0.606\n",
      "1295\t0.606\n",
      "1296\t0.606\n",
      "1297\t0.606\n",
      "1298\t0.606\n",
      "1299\t0.606\n",
      "1300\t0.606\n",
      "1301\t0.606\n",
      "1302\t0.606\n",
      "1303\t0.606\n",
      "1304\t0.606\n",
      "1305\t0.606\n",
      "1306\t0.606\n",
      "1307\t0.606\n",
      "1308\t0.606\n",
      "1309\t0.606\n",
      "1310\t0.606\n",
      "1311\t0.606\n",
      "1312\t0.606\n",
      "1313\t0.606\n",
      "1314\t0.606\n",
      "1315\t0.606\n",
      "1316\t0.606\n",
      "1317\t0.606\n",
      "1318\t0.607\n",
      "1319\t0.607\n",
      "1320\t0.607\n",
      "1321\t0.607\n",
      "1322\t0.607\n",
      "1323\t0.607\n",
      "1324\t0.607\n",
      "1325\t0.607\n",
      "1326\t0.607\n",
      "1327\t0.607\n",
      "1328\t0.607\n",
      "1329\t0.607\n",
      "1330\t0.607\n",
      "1331\t0.607\n",
      "1332\t0.607\n",
      "1333\t0.607\n",
      "1334\t0.606\n",
      "1335\t0.606\n",
      "1336\t0.606\n",
      "1337\t0.607\n",
      "1338\t0.607\n",
      "1339\t0.607\n",
      "1340\t0.607\n",
      "1341\t0.607\n",
      "1342\t0.607\n",
      "1343\t0.607\n",
      "1344\t0.607\n",
      "1345\t0.607\n",
      "1346\t0.607\n",
      "1347\t0.607\n",
      "1348\t0.607\n",
      "1349\t0.607\n",
      "1350\t0.607\n",
      "1351\t0.607\n",
      "1352\t0.607\n",
      "1353\t0.607\n",
      "1354\t0.607\n",
      "1355\t0.607\n",
      "1356\t0.607\n",
      "1357\t0.608\n",
      "1358\t0.608\n",
      "1359\t0.608\n",
      "1360\t0.608\n",
      "1361\t0.608\n",
      "1362\t0.608\n",
      "1363\t0.608\n",
      "1364\t0.608\n",
      "1365\t0.607\n",
      "1366\t0.607\n",
      "1367\t0.608\n",
      "1368\t0.608\n",
      "1369\t0.608\n",
      "1370\t0.608\n",
      "1371\t0.608\n",
      "1372\t0.608\n",
      "1373\t0.608\n",
      "1374\t0.608\n",
      "1375\t0.608\n",
      "1376\t0.608\n",
      "1377\t0.608\n",
      "1378\t0.608\n",
      "1379\t0.609\n",
      "1380\t0.608\n",
      "1381\t0.608\n",
      "1382\t0.609\n",
      "1383\t0.609\n",
      "1384\t0.609\n",
      "1385\t0.609\n",
      "1386\t0.609\n",
      "1387\t0.609\n",
      "1388\t0.609\n",
      "1389\t0.609\n",
      "1390\t0.609\n",
      "1391\t0.609\n",
      "1392\t0.609\n",
      "1393\t0.609\n",
      "1394\t0.609\n",
      "1395\t0.609\n",
      "1396\t0.609\n",
      "1397\t0.608\n",
      "1398\t0.608\n",
      "1399\t0.608\n",
      "1400\t0.608\n",
      "1401\t0.608\n",
      "1402\t0.608\n",
      "1403\t0.608\n",
      "1404\t0.608\n",
      "1405\t0.608\n",
      "1406\t0.608\n",
      "1407\t0.608\n",
      "1408\t0.608\n",
      "1409\t0.608\n",
      "1410\t0.608\n",
      "1411\t0.608\n",
      "1412\t0.608\n",
      "1413\t0.608\n",
      "1414\t0.608\n",
      "1415\t0.608\n",
      "1416\t0.608\n",
      "1417\t0.608\n",
      "1418\t0.608\n",
      "1419\t0.607\n",
      "1420\t0.607\n",
      "1421\t0.607\n",
      "1422\t0.607\n",
      "1423\t0.607\n",
      "1424\t0.607\n",
      "1425\t0.607\n",
      "1426\t0.607\n",
      "1427\t0.607\n",
      "1428\t0.607\n",
      "1429\t0.607\n",
      "1430\t0.607\n",
      "1431\t0.607\n",
      "1432\t0.606\n",
      "1433\t0.606\n",
      "1434\t0.606\n",
      "1435\t0.606\n",
      "1436\t0.606\n",
      "1437\t0.606\n",
      "1438\t0.606\n",
      "1439\t0.606\n",
      "1440\t0.606\n",
      "1441\t0.606\n",
      "1442\t0.607\n",
      "1443\t0.607\n",
      "1444\t0.607\n",
      "1445\t0.607\n",
      "1446\t0.607\n",
      "1447\t0.607\n",
      "1448\t0.607\n",
      "1449\t0.607\n",
      "1450\t0.607\n",
      "1451\t0.607\n",
      "1452\t0.607\n",
      "1453\t0.607\n",
      "1454\t0.607\n",
      "1455\t0.607\n",
      "1456\t0.607\n",
      "1457\t0.607\n",
      "1458\t0.607\n",
      "1459\t0.607\n",
      "1460\t0.607\n",
      "1461\t0.606\n",
      "1462\t0.606\n",
      "1463\t0.606\n",
      "1464\t0.606\n",
      "1465\t0.606\n",
      "1466\t0.606\n",
      "1467\t0.606\n",
      "1468\t0.606\n",
      "1469\t0.606\n",
      "1470\t0.606\n",
      "1471\t0.606\n",
      "1472\t0.606\n",
      "1473\t0.606\n",
      "1474\t0.606\n",
      "1475\t0.606\n",
      "1476\t0.606\n",
      "1477\t0.606\n",
      "1478\t0.606\n",
      "1479\t0.606\n",
      "1480\t0.606\n",
      "1481\t0.606\n",
      "1482\t0.606\n",
      "1483\t0.606\n",
      "1484\t0.606\n",
      "1485\t0.606\n",
      "1486\t0.606\n",
      "1487\t0.606\n",
      "1488\t0.606\n",
      "1489\t0.606\n",
      "1490\t0.606\n",
      "1491\t0.606\n",
      "1492\t0.606\n",
      "1493\t0.606\n",
      "1494\t0.606\n",
      "1495\t0.606\n",
      "1496\t0.606\n",
      "1497\t0.606\n",
      "1498\t0.606\n",
      "1499\t0.606\n",
      "1500\t0.606\n",
      "1501\t0.606\n",
      "1502\t0.606\n",
      "1503\t0.606\n",
      "1504\t0.606\n",
      "1505\t0.606\n",
      "1506\t0.606\n",
      "1507\t0.606\n",
      "1508\t0.606\n",
      "1509\t0.606\n",
      "1510\t0.606\n",
      "1511\t0.606\n",
      "1512\t0.606\n",
      "1513\t0.606\n",
      "1514\t0.606\n",
      "1515\t0.606\n",
      "1516\t0.606\n",
      "1517\t0.606\n",
      "1518\t0.606\n",
      "1519\t0.606\n",
      "1520\t0.606\n",
      "1521\t0.606\n",
      "1522\t0.606\n",
      "1523\t0.606\n",
      "1524\t0.606\n",
      "1525\t0.606\n",
      "1526\t0.606\n",
      "1527\t0.606\n",
      "1528\t0.606\n",
      "1529\t0.606\n",
      "1530\t0.606\n",
      "1531\t0.606\n",
      "1532\t0.606\n",
      "1533\t0.606\n",
      "1534\t0.606\n",
      "1535\t0.606\n",
      "1536\t0.606\n",
      "1537\t0.606\n",
      "1538\t0.606\n",
      "1539\t0.606\n",
      "1540\t0.606\n",
      "1541\t0.606\n",
      "1542\t0.606\n",
      "1543\t0.606\n",
      "1544\t0.606\n",
      "1545\t0.606\n",
      "1546\t0.606\n",
      "1547\t0.606\n",
      "1548\t0.606\n",
      "1549\t0.606\n",
      "1550\t0.606\n",
      "1551\t0.606\n",
      "1552\t0.606\n",
      "1553\t0.606\n",
      "1554\t0.606\n",
      "1555\t0.606\n",
      "1556\t0.606\n",
      "1557\t0.606\n",
      "1558\t0.606\n",
      "1559\t0.606\n",
      "1560\t0.606\n",
      "1561\t0.606\n",
      "1562\t0.606\n",
      "1563\t0.606\n",
      "1564\t0.606\n",
      "1565\t0.606\n",
      "1566\t0.606\n",
      "1567\t0.606\n",
      "1568\t0.606\n",
      "1569\t0.606\n",
      "1570\t0.606\n",
      "1571\t0.606\n",
      "1572\t0.606\n",
      "1573\t0.606\n",
      "1574\t0.606\n",
      "1575\t0.606\n",
      "1576\t0.606\n",
      "1577\t0.606\n",
      "1578\t0.606\n",
      "1579\t0.606\n",
      "1580\t0.606\n",
      "1581\t0.606\n",
      "1582\t0.606\n",
      "1583\t0.606\n",
      "1584\t0.606\n",
      "1585\t0.606\n",
      "1586\t0.606\n",
      "1587\t0.606\n",
      "1588\t0.606\n",
      "1589\t0.606\n",
      "1590\t0.606\n",
      "1591\t0.606\n",
      "1592\t0.606\n",
      "1593\t0.606\n",
      "1594\t0.606\n",
      "1595\t0.606\n",
      "1596\t0.606\n",
      "1597\t0.606\n",
      "1598\t0.606\n",
      "1599\t0.606\n",
      "1600\t0.606\n",
      "1601\t0.606\n",
      "1602\t0.606\n",
      "1603\t0.606\n",
      "1604\t0.606\n",
      "1605\t0.606\n",
      "1606\t0.606\n",
      "1607\t0.606\n",
      "1608\t0.606\n",
      "1609\t0.606\n",
      "1610\t0.606\n",
      "1611\t0.606\n",
      "1612\t0.606\n",
      "1613\t0.606\n",
      "1614\t0.606\n",
      "1615\t0.606\n",
      "1616\t0.606\n",
      "1617\t0.606\n",
      "1618\t0.606\n",
      "1619\t0.606\n",
      "1620\t0.606\n",
      "1621\t0.606\n",
      "1622\t0.606\n",
      "1623\t0.606\n",
      "1624\t0.606\n",
      "1625\t0.606\n",
      "1626\t0.606\n",
      "1627\t0.606\n",
      "1628\t0.606\n",
      "1629\t0.606\n",
      "1630\t0.606\n",
      "1631\t0.606\n",
      "1632\t0.606\n",
      "1633\t0.606\n",
      "1634\t0.606\n",
      "1635\t0.606\n",
      "1636\t0.606\n",
      "1637\t0.606\n",
      "1638\t0.606\n",
      "1639\t0.606\n",
      "1640\t0.606\n",
      "1641\t0.606\n",
      "1642\t0.606\n",
      "1643\t0.606\n",
      "1644\t0.606\n",
      "1645\t0.606\n",
      "1646\t0.606\n",
      "1647\t0.606\n",
      "1648\t0.606\n",
      "1649\t0.606\n",
      "1650\t0.606\n",
      "1651\t0.606\n",
      "1652\t0.606\n",
      "1653\t0.606\n",
      "1654\t0.606\n",
      "1655\t0.606\n",
      "1656\t0.606\n",
      "1657\t0.606\n",
      "1658\t0.606\n",
      "1659\t0.606\n",
      "1660\t0.606\n",
      "1661\t0.606\n",
      "1662\t0.606\n",
      "1663\t0.606\n",
      "1664\t0.606\n",
      "1665\t0.606\n",
      "1666\t0.606\n",
      "1667\t0.606\n",
      "1668\t0.606\n",
      "1669\t0.606\n",
      "1670\t0.606\n",
      "1671\t0.606\n",
      "1672\t0.606\n",
      "1673\t0.606\n",
      "1674\t0.606\n",
      "1675\t0.606\n",
      "1676\t0.606\n",
      "1677\t0.606\n",
      "1678\t0.606\n",
      "1679\t0.606\n",
      "1680\t0.606\n",
      "1681\t0.606\n",
      "1682\t0.606\n",
      "1683\t0.606\n",
      "1684\t0.606\n",
      "1685\t0.606\n",
      "1686\t0.606\n",
      "1687\t0.606\n",
      "1688\t0.606\n",
      "1689\t0.606\n",
      "1690\t0.606\n",
      "1691\t0.606\n",
      "1692\t0.606\n",
      "1693\t0.606\n",
      "1694\t0.606\n",
      "1695\t0.606\n",
      "1696\t0.606\n",
      "1697\t0.606\n",
      "1698\t0.606\n",
      "1699\t0.606\n",
      "1700\t0.606\n",
      "1701\t0.606\n",
      "1702\t0.606\n",
      "1703\t0.606\n",
      "1704\t0.606\n",
      "1705\t0.606\n",
      "1706\t0.606\n",
      "1707\t0.606\n",
      "1708\t0.606\n",
      "1709\t0.606\n",
      "1710\t0.606\n",
      "1711\t0.606\n",
      "1712\t0.606\n",
      "1713\t0.606\n",
      "1714\t0.606\n",
      "1715\t0.606\n",
      "1716\t0.606\n",
      "1717\t0.606\n",
      "1718\t0.606\n",
      "1719\t0.606\n",
      "1720\t0.606\n",
      "1721\t0.606\n",
      "1722\t0.606\n",
      "1723\t0.606\n",
      "1724\t0.606\n",
      "1725\t0.606\n",
      "1726\t0.606\n",
      "1727\t0.606\n",
      "1728\t0.606\n",
      "1729\t0.606\n",
      "1730\t0.606\n",
      "1731\t0.606\n",
      "1732\t0.606\n",
      "1733\t0.606\n",
      "1734\t0.606\n",
      "1735\t0.606\n",
      "1736\t0.606\n",
      "1737\t0.606\n",
      "1738\t0.606\n",
      "1739\t0.606\n",
      "1740\t0.606\n",
      "1741\t0.606\n",
      "1742\t0.606\n",
      "1743\t0.606\n",
      "1744\t0.606\n",
      "1745\t0.606\n",
      "1746\t0.606\n",
      "1747\t0.606\n",
      "1748\t0.606\n",
      "1749\t0.606\n",
      "1750\t0.606\n",
      "1751\t0.606\n",
      "1752\t0.606\n",
      "1753\t0.606\n",
      "1754\t0.606\n",
      "1755\t0.606\n",
      "1756\t0.606\n",
      "1757\t0.606\n",
      "1758\t0.606\n",
      "1759\t0.606\n",
      "1760\t0.606\n",
      "1761\t0.606\n",
      "1762\t0.606\n",
      "1763\t0.606\n",
      "1764\t0.606\n",
      "1765\t0.606\n",
      "1766\t0.606\n",
      "1767\t0.606\n",
      "1768\t0.606\n",
      "1769\t0.606\n",
      "1770\t0.606\n",
      "1771\t0.606\n",
      "1772\t0.606\n",
      "1773\t0.606\n",
      "1774\t0.606\n",
      "1775\t0.606\n",
      "1776\t0.606\n",
      "1777\t0.606\n",
      "1778\t0.606\n",
      "1779\t0.606\n",
      "1780\t0.606\n",
      "1781\t0.606\n",
      "1782\t0.606\n",
      "1783\t0.606\n",
      "1784\t0.606\n",
      "1785\t0.606\n",
      "1786\t0.606\n",
      "1787\t0.606\n",
      "1788\t0.606\n",
      "1789\t0.606\n",
      "1790\t0.606\n",
      "1791\t0.606\n",
      "1792\t0.606\n",
      "1793\t0.606\n",
      "1794\t0.606\n",
      "1795\t0.606\n",
      "1796\t0.606\n",
      "1797\t0.606\n",
      "1798\t0.606\n",
      "1799\t0.606\n",
      "best max for leaves is 41  with an accuracy of  0.6854543832761655\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation for different tree sizes\n",
    "\n",
    "print('Leaves\\tMean accuracy')\n",
    "print('---------------------')\n",
    "\n",
    "# initiate best max and accuracy as 0 and 0\n",
    "best_max_leaves = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# try all leaves from 0-1800\n",
    "for num_leaves in range(0,1800,1):\n",
    "\n",
    "        # Trees must have at least 2 leaves\n",
    "        if num_leaves >= 2:\n",
    "\n",
    "            # construct a classifier with a limit on its number of leaves\n",
    "            dtc = DecisionTreeClassifier(max_leaf_nodes=num_leaves, random_state=5).fit(X_train, Y_train['two_year_recid'])\n",
    "            \n",
    "            # Get validation accuracy via 5-fold cross-validation\n",
    "            scores = cross_val_score(dtc, X_train, Y_train['two_year_recid'])\n",
    "        \n",
    "        print(\"{}\\t{:.3f}\".format(num_leaves,scores.mean()))\n",
    "\n",
    "        # compare accuracy, update best max leaves and best accuracy if better than current\n",
    "        if scores.mean() > best_accuracy:\n",
    "              best_max_leaves = num_leaves\n",
    "              best_accuracy = scores.mean()\n",
    "\n",
    "print(\"best max for leaves is\", best_max_leaves, \" with an accuracy of \", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46824a-f7f5-4408-94fb-8c463c5a7e0f",
   "metadata": {},
   "source": [
    "Adjust the range of values for `max_leaf_nodes` in the cell above, to identify the best value.\n",
    "\n",
    "### Part 2, Step 3: Calculate the selected model's test performance\n",
    "\n",
    "Train a decision tree using your selected value of `max_leaf_nodes` on the full training set. Assess its accuracy on your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "502349cb-63ee-4146-8c5e-f58b26241165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "Trained decision tree with 41 leaves and test accuracy 0.66.\n"
     ]
    }
   ],
   "source": [
    "max_leaves = best_max_leaves\n",
    "\n",
    "# Create a model w/ best max leaves\n",
    "dtc = DecisionTreeClassifier(max_leaf_nodes=max_leaves)\n",
    "\n",
    "# Fit model to training data\n",
    "dtc = dtc.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "# Evaluate training accuracy\n",
    "prediction_values = dtc.predict(X_test)\n",
    "\n",
    "# average by counting \"1\"s (accurate results) and dividing by total number of observations\n",
    "count = 0\n",
    "for i in range(prediction_values.size):\n",
    "    if prediction_values[i] == Y_test['two_year_recid'].iloc[i]:\n",
    "        count = count + 1\n",
    "\n",
    "dtc_accuracy = count / prediction_values.size\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = dtc.get_n_leaves()\n",
    "\n",
    "print(num_leaves)\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and test accuracy {:.2f}.'.format(num_leaves, dtc_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c6b58-4f9e-4d22-b9c0-914b22aea0f9",
   "metadata": {},
   "source": [
    "# Part 3: Auditing a decision tree for demographic biases\n",
    "\n",
    ">Your training data includes several demographic variables (i.e., age, sex, race). A crude way to assess whether a model has some demographic bias is to remove the corresponding variables from your training data and explore how that removal affects your model's performance. Decision trees have the advantage of being interpretable machine learning models. By going through the decision nodes (i.e., branching points), you can \"open the black box and look inside\". Specifically, you can assess how each feature is used in the decision making process.\n",
    ">\n",
    ">This part includes two steps:\n",
    ">\n",
    ">1. Check for racial bias via performance assessment\n",
    ">2. Check for racial bias via decision rules\n",
    "  \n",
    "### Part 3, Step 2: Check for racial bias via performance assessment\n",
    "A simple approach to identifying demographic biases in machine learning is the following: (i) Train and validate the model on the full training set, (ii) train and validate the model on a subset of training variables that excludes the variables related to a potential demographic bias, (iii) compare the results. \n",
    "\n",
    "You have noticed that the validation accuracy of your model can vary for different holdout set selections. To account for these variations, you are going to compare the mean validation accuracy over 100 trees. (You have completed (i) in the previous cell already. Continue now with (ii).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "25f5e4e1-5487-4220-afcc-eeb1e228c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained decision tree with 41 leaves and test accuracy 0.68.\n"
     ]
    }
   ],
   "source": [
    "# Create subset of training data without information on race. \n",
    "# (The information on race was encoded in the one-hot features.)\n",
    "remaining_features = [v for v in X.columns if v not in one_hot_features]\n",
    "X_train_sub = X_train[remaining_features]\n",
    "X_test_sub = X_test[remaining_features]\n",
    "\n",
    "# Create a model\n",
    "dtc_sub = DecisionTreeClassifier(max_leaf_nodes=best_max_leaves)\n",
    "    \n",
    "# Fit model to training data\n",
    "dtc_sub.fit(X_train_sub, Y_train['two_year_recid'])\n",
    "\n",
    "# Evaluate training accuracy\n",
    "y_pred = dtc_sub.predict(X_test_sub)\n",
    "accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "\n",
    "# cross-validate over 100 times\n",
    "scores = cross_val_score(dtc_sub, X_train_sub, Y_train['two_year_recid'])\n",
    "\n",
    "# compute accuracy for scores from each iteration\n",
    "accuracy = scores.mean()\n",
    "\n",
    "# Check size of decision tree\n",
    "num_leaves = dtc_sub.get_n_leaves()\n",
    "\n",
    "# Report results\n",
    "print('Trained decision tree with {} leaves and test accuracy {:.2f}.'.format(num_leaves, accuracy))\n",
    "\n",
    "# The mean accuracy for the subselected feature set (~0.68) is slightly better than the mean accuracy for all-features set (~0.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e10fce-64ad-4f3c-95db-9e31a5e33568",
   "metadata": {},
   "source": [
    "Comparing the mean accuracy values on the all features versus the subselected feature set, what do you conclude about the importance of racial information in this classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18654ba4-834b-49d8-b110-712844d5bf62",
   "metadata": {},
   "source": [
    "### Part 2, Step 3: Check for racial bias via decision rules\n",
    "The interpretability of decision trees allows for an alternative approach to detecting racial bias. You can simply look at the decision rules. Use the scit-kit learn's function `export_text` to get your decision tree in text format. Compare the decision rules of the your tree with all features and your tree fitted on the subset without racial information. Do you find any indication of racial bias in the decision rules of the first tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "212a88fb-f48a-4081-87df-eb1ce5b69bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- priors_count <= 2.50\n",
      "|   |--- age <= 28.50\n",
      "|   |   |--- age <= 22.50\n",
      "|   |   |   |--- age <= 20.50\n",
      "|   |   |   |   |--- priors_count <= 0.50\n",
      "|   |   |   |   |   |--- juv_other_count <= 1.50\n",
      "|   |   |   |   |   |   |--- age <= 19.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- age >  19.50\n",
      "|   |   |   |   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- juv_other_count >  1.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- priors_count >  0.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- age >  20.50\n",
      "|   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |--- age <= 21.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- age >  21.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |--- priors_count <= 1.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- priors_count >  1.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |--- age >  22.50\n",
      "|   |   |   |--- priors_count <= 0.50\n",
      "|   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- priors_count >  0.50\n",
      "|   |   |   |   |--- priors_count <= 1.50\n",
      "|   |   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |   |--- age <= 26.50\n",
      "|   |   |   |   |   |   |   |--- race_is_African-American <= 0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- race_is_African-American >  0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- age >  26.50\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- priors_count >  1.50\n",
      "|   |   |   |   |   |--- age <= 24.50\n",
      "|   |   |   |   |   |   |--- race_is_African-American <= 0.50\n",
      "|   |   |   |   |   |   |   |--- c_charge_degree <= 0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- c_charge_degree >  0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- race_is_African-American >  0.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- age >  24.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |--- age >  28.50\n",
      "|   |   |--- priors_count <= 0.50\n",
      "|   |   |   |--- age <= 52.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- age >  52.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- priors_count >  0.50\n",
      "|   |   |   |--- age <= 32.50\n",
      "|   |   |   |   |--- race_is_Caucasian <= 0.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- race_is_Caucasian >  0.50\n",
      "|   |   |   |   |   |--- age <= 30.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- age >  30.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- age >  32.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|--- priors_count >  2.50\n",
      "|   |--- age <= 33.50\n",
      "|   |   |--- priors_count <= 7.50\n",
      "|   |   |   |--- age <= 27.50\n",
      "|   |   |   |   |--- c_charge_degree <= 0.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- c_charge_degree >  0.50\n",
      "|   |   |   |   |   |--- age <= 23.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- age >  23.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- age >  27.50\n",
      "|   |   |   |   |--- priors_count <= 5.50\n",
      "|   |   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |   |--- race_is_African-American <= 0.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- race_is_African-American >  0.50\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- priors_count >  5.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |--- priors_count >  7.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |--- age >  33.50\n",
      "|   |   |--- priors_count <= 6.50\n",
      "|   |   |   |--- age <= 48.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- age >  48.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- priors_count >  6.50\n",
      "|   |   |   |--- priors_count <= 20.50\n",
      "|   |   |   |   |--- age <= 59.50\n",
      "|   |   |   |   |   |--- juv_misd_count <= 3.50\n",
      "|   |   |   |   |   |   |--- juv_other_count <= 0.50\n",
      "|   |   |   |   |   |   |   |--- age <= 34.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- age >  34.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- juv_other_count >  0.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- juv_misd_count >  3.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- age >  59.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- priors_count >  20.50\n",
      "|   |   |   |   |--- class: 1\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "|--- priors_count <= 2.50\n",
      "|   |--- age <= 28.50\n",
      "|   |   |--- age <= 22.50\n",
      "|   |   |   |--- age <= 20.50\n",
      "|   |   |   |   |--- priors_count <= 0.50\n",
      "|   |   |   |   |   |--- juv_other_count <= 1.50\n",
      "|   |   |   |   |   |   |--- age <= 19.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- age >  19.50\n",
      "|   |   |   |   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- juv_other_count >  1.50\n",
      "|   |   |   |   |   |   |--- juv_other_count <= 2.50\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- juv_other_count >  2.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- priors_count >  0.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- age >  20.50\n",
      "|   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |--- age <= 21.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- age >  21.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |--- priors_count <= 1.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- priors_count >  1.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |--- age >  22.50\n",
      "|   |   |   |--- priors_count <= 0.50\n",
      "|   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- priors_count >  0.50\n",
      "|   |   |   |   |--- priors_count <= 1.50\n",
      "|   |   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |   |--- age <= 26.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- age >  26.50\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- priors_count >  1.50\n",
      "|   |   |   |   |   |--- age <= 24.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- age >  24.50\n",
      "|   |   |   |   |   |   |--- juv_misd_count <= 0.50\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- juv_misd_count >  0.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |--- age >  28.50\n",
      "|   |   |--- priors_count <= 0.50\n",
      "|   |   |   |--- age <= 52.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- age >  52.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- priors_count >  0.50\n",
      "|   |   |   |--- age <= 32.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- age >  32.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|--- priors_count >  2.50\n",
      "|   |--- age <= 33.50\n",
      "|   |   |--- priors_count <= 7.50\n",
      "|   |   |   |--- age <= 27.50\n",
      "|   |   |   |   |--- c_charge_degree <= 0.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- c_charge_degree >  0.50\n",
      "|   |   |   |   |   |--- age <= 23.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- age >  23.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- age >  27.50\n",
      "|   |   |   |   |--- priors_count <= 5.50\n",
      "|   |   |   |   |   |--- sex <= 0.50\n",
      "|   |   |   |   |   |   |--- age <= 32.50\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- age >  32.50\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- sex >  0.50\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- priors_count >  5.50\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |--- priors_count >  7.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |--- age >  33.50\n",
      "|   |   |--- priors_count <= 6.50\n",
      "|   |   |   |--- age <= 48.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- age >  48.50\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |--- priors_count >  6.50\n",
      "|   |   |   |--- priors_count <= 20.50\n",
      "|   |   |   |   |--- age <= 59.50\n",
      "|   |   |   |   |   |--- juv_misd_count <= 3.50\n",
      "|   |   |   |   |   |   |--- juv_other_count <= 0.50\n",
      "|   |   |   |   |   |   |   |--- age <= 34.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- age >  34.50\n",
      "|   |   |   |   |   |   |   |   |--- priors_count <= 9.50\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- priors_count >  9.50\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- juv_other_count >  0.50\n",
      "|   |   |   |   |   |   |   |--- priors_count <= 19.50\n",
      "|   |   |   |   |   |   |   |   |--- age <= 49.50\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- age >  49.50\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- priors_count >  19.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- juv_misd_count >  3.50\n",
      "|   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- age >  59.50\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |--- priors_count >  20.50\n",
      "|   |   |   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(export_text(dtc, feature_names = X_train.columns))\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(export_text(dtc_sub, feature_names = X_train_sub.columns))\n",
    "# One obvious difference when comparing the two is having decision rules based on race in the first tree which are absent in the second.\n",
    "# Specifically, when examining the decision rules of the first tree, I found a several instances of racial bias.\n",
    "# If a subject is between 22.5 and 26.5, has a priors count between 0.5 and 1.5, and is male, then being African-American determines whether the subject\n",
    "# is predicted to recidivism within 2 years: if African-American, the decision tree predicts yes, if not, the decision tree predicts no.\n",
    "# Another example: If a subject is between 22.5 and 24.5, has a priors count between 1.5 and 2.5, then being African-American determines whether the subject\n",
    "# is predicted to recidivism within 2 years: if African-American, the decision tree predicts yes, if not, the decision tree predicts no.\n",
    "# I found four examples of decision rules making different outcomes based on race (3 times if African-American, 1 time based on whether the subject was white)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82dfe8-9fd7-44f9-aca4-8e33f9203798",
   "metadata": {},
   "source": [
    "# Part 4: Comparison to other linear classifiers\n",
    "\n",
    ">For some types of data, decision trees tend to achieve lower prediction accuracies In this part, you will train and tune several classifiers on the COMPAS data. You will then compare their performance on your test set.\n",
    ">\n",
    ">This part includes three steps:\n",
    ">\n",
    ">1. Fit LDA and logistic regression\n",
    ">2. Tune and fit ensemble methods\n",
    ">3. Tune and fit SVC\n",
    ">4. Compare test accuracy of all your models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e97edaf2-04da-4f12-aa8f-75847bd6e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA accuracy:  0.63915857605178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg accuracy:  0.6423948220064725\n",
      "RandomForest accuracy:  0.627831715210356 with best n  50\n",
      "Bagging accuracy:  0.6310679611650486 with best n 50\n",
      "GradientBoosting accuracy:  0.6715210355987055  with learning rate  0.02\n",
      "SVC accuracy (best C):  0.6618122977346278\n",
      "Best method is: GradientBoosting , with accuracy: 0.6715210355987055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# make dictionary to compare all methods\n",
    "all_accuracies = {}\n",
    "# add our decision tree\n",
    "all_accuracies[\"DTC\"] = dtc_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "# Evaluate training accuracy and print\n",
    "y_pred = lda.predict(X_test)\n",
    "accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "print(\"LDA accuracy: \", accuracy)\n",
    "# add to dictionary of all test methods\n",
    "all_accuracies[\"LDA\"] = accuracy\n",
    "\n",
    "\n",
    "# fit logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "# Evaluate training accuracy and print\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "print(\"LogReg accuracy: \", accuracy)\n",
    "all_accuracies[\"LogReg\"] = accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Tune and fit ensemble methods\n",
    "# Random Forest\n",
    "best_accuracy = 0\n",
    "best_n = 0\n",
    "estimator_range = 200\n",
    "increment = 10\n",
    "start = 10\n",
    "# tune based on number of estimators (n_estimators)\n",
    "for i in range(start,estimator_range,increment):\n",
    "    rand_for = RandomForestClassifier(n_estimators=i, random_state=5)\n",
    "    rand_for.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "    # Evaluate training accuracy and print\n",
    "    y_pred = rand_for.predict(X_test)\n",
    "    accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_n = i\n",
    "\n",
    "print(\"RandomForest accuracy: \", best_accuracy, \"with best n \", best_n)\n",
    "all_accuracies[\"RandomForest\"] = best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bagging\n",
    "best_accuracy = 0\n",
    "best_n = 0\n",
    "estimator_range = 200\n",
    "increment = 10\n",
    "start = 10\n",
    "# tune based on number of estimators (n_estimators)\n",
    "for i in range(start,estimator_range,increment):\n",
    "    bag = BaggingClassifier(n_estimators=i, random_state=5)\n",
    "    bag.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "    # Evaluate training accuracy and print\n",
    "    y_pred = bag.predict(X_test)\n",
    "    accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_n = i\n",
    "\n",
    "print(\"Bagging accuracy: \", best_accuracy, \"with best n\", best_n)\n",
    "all_accuracies[\"Bagging\"] = best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GradientBoosting\n",
    "best_accuracy = 0\n",
    "best_lr = 0\n",
    "lr_range = 100\n",
    "# tune based on learning rate (0.01-1 in increments of 0.01)\n",
    "for i in range(lr_range):\n",
    "    lr = 0.01 * i\n",
    "    GradBoost = GradientBoostingClassifier(learning_rate=lr, random_state=5)\n",
    "    GradBoost.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "    # Evaluate training accuracy and print\n",
    "    y_pred = GradBoost.predict(X_test)\n",
    "    accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_lr = i*0.01\n",
    "print(\"GradientBoosting accuracy: \", best_accuracy, \" with learning rate \", best_lr)\n",
    "all_accuracies[\"GradientBoosting\"] = best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Tune + fit SVC\n",
    "# tune for C = 0.01, 0.1, 1, 10, 100\n",
    "best_accuracy = 0\n",
    "best_C = 0\n",
    "powers = 5\n",
    "for i in range(powers):\n",
    "    # adjust C value (increase by power of 10 each time)\n",
    "    C_value = 1 * 10**(-2 + i)\n",
    "    svc = SVC(C=C_value)\n",
    "    svc.fit(X_train, Y_train['two_year_recid'])\n",
    "\n",
    "    # Evaluate training accuracy and print\n",
    "    y_pred = svc.predict(X_test)\n",
    "    accuracy = (y_pred == Y_test['two_year_recid']).mean()\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_C = C_value\n",
    "\n",
    "print(\"SVC accuracy (best C): \", best_accuracy)\n",
    "all_accuracies[\"SVC\"] = best_accuracy\n",
    "\n",
    "\n",
    "overall_best_accuracy = 0\n",
    "best_method = \"\"\n",
    "for i in all_accuracies:\n",
    "    if all_accuracies[i] > overall_best_accuracy:\n",
    "        overall_best_accuracy = all_accuracies[i]\n",
    "        best_method = i\n",
    "\n",
    "print(\"Best method is:\", best_method, \", with accuracy:\", overall_best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
