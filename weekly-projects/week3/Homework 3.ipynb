{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 3 (due 07/16/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c07e6-1b27-42dc-b58a-8c0b54d8eb0f",
   "metadata": {},
   "source": [
    "# Eigenbears\n",
    "\n",
    "## Instructions\n",
    "\n",
    "#### Part 1: Data preparation (ALREADY COMPLETED)\n",
    "The folder that contains this notebook also contains a data set of pictures of pandas and brown bears. \n",
    "1. Load the images for train and test.\n",
    "2. Reduce their size/resolution by 50%.\n",
    "3. Convert the colors to grey scale.\n",
    "4. Display a few of the images. \n",
    "\n",
    "#### Part 2: Singular value decomposition (FOR YOU TO COMPLETE)\n",
    "1. Follow the steps in the eigenfaces tutorial from the UW databook [[link]](https://github.com/dynamicslab/databook_python/blob/master/CH01/CH01_SEC06_2_3_4.ipynb) to perform an SVD on the images.\n",
    "2. Note that you will need to \"center the data\" before doing the SVD. Data centering means replacing each variable $X_i$ with a new variable $X_i'$ that is equal to $X_i$ minus $X_i$'s mean value. (Think carefully about whether you want to use the mean of train set, test set, or the full data set for this.)\n",
    "3. Display the first four \"eigenbears\" (i.e., the images associated with the first four eigenvectors). Explain what you see.\n",
    "4. The singular vectors are sometimes called \"principal components\". Principal component analysis (PCA) is a data analysis method for which one projects high-dimensional data into the subspace of the first two singular vectors. Use the code scaffold provided below do perform PCA for the bears data. What do you notice?\n",
    "\n",
    "#### Part 3: Non-parametric supervised classification (PARTIALLY FOR YOU TO COMPLETE)\n",
    "1. Build a $k$-nearest-neighbors model with the train set, and test its accuracy on the test set. (ALREADY COMPLETE)\n",
    "2. Try different values of $k$ between 1 and 15. For what value do you get the best test accuracy? (FOR YOU TO COMPLETE)\n",
    "3. Which bears seem to be hard to classify? Display them.\n",
    "4. What might make them hard to classify?\n",
    "\n",
    "#### Part 4: Parametric supervised classification (PARTIALLY FOR YOU TO COMPLETE)\n",
    "1. Try using logistic regression and LDA to classify the bears. \n",
    "2. What method gives you the best test accuracy? \n",
    "3. How does the result compare to the non-parametric classification?\n",
    "4. One can use the absolute values of the regression coefficients to see which pixels have the greatest influence on the prediction of the logistic regression. Retrieving the coefficients $\\beta_i$ for each pixel $X_i$ and displaying them as an image creates a \"bear mask\". This notebook includes code for creating that bear mask. Take a look at the bear mask and explain what you see.\n",
    "\n",
    "#### Part 5: Robustness to additive white noise  (FOR YOU TO RUN AND COMMENT ON)\n",
    "1. Rerun the code with `add_noise` set to True. The images of the bears should now be very noisy.\n",
    "2. How does the additive noise affect the test accuracy of the various models and why?\n",
    "3. How does additive noise affect the eigenbears and the bear mask?\n",
    "4. Can you think of other types of noise that might affect the classification results differently?\n",
    "\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35984cf0-ad0b-4a99-bb7b-4b2aaa26f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9a332-5d32-4dd1-85b0-fceb1c50ad75",
   "metadata": {},
   "source": [
    "## Part 1: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e39ed3-c685-487f-a1e3-8c7d1ecace23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle settings\n",
    "add_noise = False\n",
    "\n",
    "# Initiliaze lists for image collection\n",
    "train_images = []\n",
    "test_images = []\n",
    "\n",
    "for i, images in enumerate([train_images, test_images]):\n",
    "\n",
    "    # set paths to images of pandas and bears in train and test set\n",
    "    datasetname = ['Train','Test'][i]\n",
    "    folder_path1 = 'PandasBears/{}/Pandas/'.format(datasetname)\n",
    "    folder_path2 = 'PandasBears/{}/Bears/'.format(datasetname)\n",
    "    \n",
    "\n",
    "    for folder_path in [folder_path1, folder_path2]:\n",
    "\n",
    "        # print the name of the folder that is currently being processed\n",
    "        print(folder_path, end=' ')\n",
    "        \n",
    "        # go through all files in the folder\n",
    "        file_count = 0\n",
    "        for filename in os.listdir(folder_path):\n",
    "            \n",
    "            # find the files that are JPEGs\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "\n",
    "                # add 1 to the file count\n",
    "                file_count += 1\n",
    "                \n",
    "                # Construct full file path\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "                # import image\n",
    "                image = plt.imread(file_path, format='jpeg')\n",
    "    \n",
    "                # convert to gray scale\n",
    "                image = np.dot(image[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "    \n",
    "                # decrease image size by 50%\n",
    "                image = image[::2,::2]\n",
    "\n",
    "                if add_noise:\n",
    "                    # add some noise\n",
    "                    image = image + np.random.normal(scale=100, size=image.shape)\n",
    "    \n",
    "                # add the new image to collection\n",
    "                images.append(image)\n",
    "\n",
    "        print('has {} images'.format(file_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38f5fc-e3e5-487c-abd7-2247584a9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at 4 random bears\n",
    "for i0, i in enumerate(np.random.randint(0, 500, size=4)):\n",
    "    plt.subplot(2,2,1+i0)\n",
    "    plt.imshow(train_images[i][::2,::2],cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12068bdf-3757-45f9-a79d-9df5cd714901",
   "metadata": {},
   "source": [
    "# Part 2: Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ace9a-0e7e-4423-9a56-665d33380f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct data matrix of centered data\n",
    "'''ADD SOME CODE HERE'''\n",
    "\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vh = np.linalg.svd(A) # replace A with your centered data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877cbb2-9eb0-4eb5-9156-cef1aeaae42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first four eigenbears\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,1+i)\n",
    "    plt.imshow((U[:,i]).reshape((128,128)), cmap='Greys_r',\n",
    "        # force colormap to be the same for all four\n",
    "        vmin=-np.max(np.abs(U[:,:4])),\n",
    "        vmax=np.max(np.abs(U[:,:4])))\n",
    "    plt.colorbar()\n",
    "plt.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a60963-7710-4b53-9fcf-d077fe714786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering of panda bears and brown bears along the first and second principal component\n",
    "# indices of pandas in the test set\n",
    "indices_pandas = range(50)\n",
    "# indices of brown bears in the test set\n",
    "indices_brownbears = range(50,100)\n",
    "\n",
    "for i, indices in enumerate([indices_pandas, indices_brownbears]):\n",
    "    # get projections of data onto principal component 1\n",
    "    p1 = [np.dot(U[:,0],np.ravel(test_images(x))) for x in indices]\n",
    "    # get projections of data onto principal component 2\n",
    "    p2 = [np.dot(U[:,1],np.ravel(test_images(x))) for x in indices]\n",
    "    plt.plot(p1, p2, marker='+x'[i], lw=0, label=['Pandas', 'Grizzlies'][i])\n",
    "\n",
    "# annotate axes\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "# add legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df663be5-28f9-439c-958c-9d6aa3372f1d",
   "metadata": {},
   "source": [
    "# Part 3: Nonparametric classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7142d5d-950b-4696-849b-66e029e7735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a34d6-78b5-4be2-b913-ca4228afde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct response variable: Train set was created by appending 250 pandas\n",
    "# and THEN 250 brown bears to the list of training images. We code pandas as\n",
    "# '0' and brown bears as '1'.\n",
    "y_train =  np.concatenate([np.zeros(250), np.ones(250)])\n",
    "\n",
    "# Test set was created by appending 50 pandas and THEN 50 brown bears to the \n",
    "# list of test images. We code pandas as '0' and brown bears as '1'.\n",
    "y_test = np.concatenate([np.zeros(50), np.ones(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb8028-d03f-460b-b9d9-c02bfef236b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   k\\t|  # errors\\t| misclassified bears')\n",
    "print('--------------------------------------------')\n",
    "for k in range(1,16):\n",
    "    # fit KNN model\n",
    "    modelKN = KNeighborsClassifier(n_neighbors=k).fit(A.T, y_train)\n",
    "    # use model to make predictions on the test set\n",
    "    predictions = [modelKN.predict([np.ravel(test_images[i])]) for i in len(y_test)]\n",
    "    # detect misclassifications\n",
    "    errors = np.abs((np.array(predictions).T)[0]-y_test)\n",
    "    # print results to table\n",
    "    print('    {}\\t|      {} \\t| {}'.format(k, int(np.sum(errors)), (np.argwhere(errors).T)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d31eba-121d-4c39-be32-dc4e31b3b7f7",
   "metadata": {},
   "source": [
    "# Part 4: Parametric classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cad757-6a21-4825-bd14-8868555a9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53fcc9-ca6b-41ef-8eb1-ba1545107e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ADD SOME CODE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5698a-ef5f-44de-a9cf-57b5a255720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the bear mask:\n",
    "# Show absolute value of logistic-regression coefficients for each pixel\n",
    "plt.imshow(np.abs((modelLogReg.coef_).reshape((128,128))))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621c1a7-8cf6-4b14-97c0-6d3f02e89a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
